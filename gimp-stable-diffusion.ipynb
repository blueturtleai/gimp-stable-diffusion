{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/opencoca/gimp-stable-diffusion/blob/main/gimp-stable-diffusion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU52ZvES6-1T"
      },
      "source": [
        "## GIMP Stable Diffusion v1.0.3\n",
        "\n",
        "###**IMPORTANT: Please use v1.0.3 of the GIMP plugin (current version in the github repository)** \n",
        "###The plugin version can be found at the top of the plugin file \"gimp-stable-diffusion.py\" which is located in your GIMP plugins folder.\n",
        "\n",
        "Stable Diffusion notebook, which can be used together with the GIMP plugin to generate images.\n",
        "\n",
        "Images are created with CompVis/Stability [Stable Diffusion](https://github.com/CompVis/stable-diffusion) with bonus [KLMS sampling](https://github.com/crowsonkb/k-diffusion.git) from [@RiversHaveWings](https://twitter.com/RiversHaveWings)\n",
        "\n",
        "You need to get the ckpt file and put it on your Google Drive first to use this. It can be downloaded from [HuggingFace](https://huggingface.co/CompVis/stable-diffusion).\n",
        "\n",
        "By [@pharmapsychotic](https://twitter.com/pharmapsychotic). Modified by [@BlueTurtleAI](https://twitter.com/BlueTurtleAI) for the usage with the GIMP plugin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc5OwvKdjRJF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeqQ7pt1zdI7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Mount Google Drive and Prepare Folders\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "outputs_path = \"/content/gdrive/MyDrive/AI/StableDiffusion\"\n",
        "!mkdir -p $outputs_path\n",
        "print(f\"Outputs will be saved to {outputs_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCiglGzMaFfY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!pip install pytorch-lightning torch-fidelity\n",
        "!pip install numpy omegaconf einops kornia pytorch-lightning\n",
        "!pip install albumentations transformers\n",
        "!pip install ftfy jsonmerge resize-right torchdiffeq tqdm\n",
        "#!pip install pyngrok\n",
        "#!pip install flask-ngrok\n",
        "\n",
        "!pip install flask-cloudflared\n",
        "\n",
        "!git clone https://github.com/CompVis/stable-diffusion\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!git clone https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/crowsonkb/k-diffusion.git\n",
        "\n",
        "!echo '' > ./k-diffusion/k_diffusion/__init__.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcHsbr3hblrk"
      },
      "outputs": [],
      "source": [
        "#@title Load Model\n",
        "\n",
        "#@markdown You need to get the model weights yourself and put on Google Drive or this Colab instance\n",
        "checkpoint_model_file = \"/content/gdrive/MyDrive/AI/models/sd-v1-4.ckpt\" #@param {type:\"string\"}\n",
        "\n",
        "checkpoint_model_url = \"https://ipfs.io/ipfs/bafybeicrdgunwfjxm5yr7qqe5kgybaog65wnonymaeumzkto4eagrvwz2a/stable-diffusion-v1.4-and-license.zip\"\n",
        "\n",
        "import argparse, gc, json, os, random, sys, time, glob, requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import PIL\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from einops import rearrange, repeat\n",
        "from IPython.display import display, clear_output\n",
        "from itertools import islice\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from pytorch_lightning import seed_everything\n",
        "from torch import autocast\n",
        "\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append('./k-diffusion')\n",
        "sys.path.append('./stable-diffusion')\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.models.diffusion.plms import PLMSSampler\n",
        "\n",
        "from k_diffusion.sampling import sample_lms\n",
        "from k_diffusion.external import CompVisDenoiser\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "class config():\n",
        "    def __init__(self):\n",
        "        self.ckpt = checkpoint_model_file\n",
        "        self.config = 'stable-diffusion/configs/stable-diffusion/v1-inference.yaml'\n",
        "        self.ddim_eta = 0.0\n",
        "        self.ddim_steps = 100\n",
        "        self.fixed_code = True\n",
        "        self.init_img = None\n",
        "        self.n_samples = 1\n",
        "        self.outdir = \"\"\n",
        "        self.precision = 'autocast'\n",
        "        self.prompt = \"\"\n",
        "        self.sampler = 'klms'\n",
        "        self.scale = 7.5\n",
        "        self.seed = 42\n",
        "        self.strength = 0.75 # strength for noising/unnoising. 1.0 corresponds to full destruction of information in init image\n",
        "        self.H = 512\n",
        "        self.W = 512\n",
        "        self.C = 4\n",
        "        self.f = 8\n",
        "\n",
        "class CFGDenoiser(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.inner_model = model\n",
        "\n",
        "    def forward(self, x, sigma, uncond, cond, cond_scale):\n",
        "        x_in = torch.cat([x] * 2)\n",
        "        sigma_in = torch.cat([sigma] * 2)\n",
        "        cond_in = torch.cat([uncond, cond])\n",
        "        uncond, cond = self.inner_model(x_in, sigma_in, cond=cond_in).chunk(2)\n",
        "        return uncond + (cond - uncond) * cond_scale\n",
        "\n",
        "def load_model_from_config(config, ckpt, verbose=False):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    if \"global_step\" in pl_sd:\n",
        "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    if len(m) > 0 and verbose:\n",
        "        print(\"missing keys:\")\n",
        "        print(m)\n",
        "    if len(u) > 0 and verbose:\n",
        "        print(\"unexpected keys:\")\n",
        "        print(u)\n",
        "\n",
        "    model = model.half().to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "      \n",
        "def load_img(path, shape):\n",
        "    if path.startswith('http://') or path.startswith('https://'):\n",
        "        image = Image.open(requests.get(path, stream=True).raw).convert('RGB')\n",
        "    else:\n",
        "        if os.path.isdir(path):\n",
        "            files = [file for file in os.listdir(path) if file.endswith('.png') or file .endswith('.jpg')]\n",
        "            path = os.path.join(path, random.choice(files))\n",
        "            print(f\"Chose random init image {path}\")\n",
        "        image = Image.open(path).convert('RGB')\n",
        "    image = image.resize(shape, resample=PIL.Image.LANCZOS)\n",
        "    image = np.array(image).astype(np.float16) / 255.0\n",
        "    image = image[None].transpose(0, 3, 1, 2)\n",
        "    image = torch.from_numpy(image)\n",
        "    return 2.*image - 1.\n",
        "\n",
        "opt = config()\n",
        "config = OmegaConf.load(f\"{opt.config}\")\n",
        "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
        "model = model.to(device)\n",
        "batch_idx = 0\n",
        "sample_idx = 0\n",
        "\n",
        "def generate(opt):\n",
        "    global sample_idx\n",
        "    seed_everything(opt.seed)\n",
        "    os.makedirs(opt.outdir, exist_ok=True)\n",
        "\n",
        "    if opt.sampler == 'plms':\n",
        "        sampler = PLMSSampler(model)\n",
        "    else:\n",
        "        sampler = DDIMSampler(model)\n",
        "\n",
        "    model_wrap = CompVisDenoiser(model)       \n",
        "    batch_size = opt.n_samples\n",
        "    prompt = opt.prompt\n",
        "    assert prompt is not None\n",
        "    data = [batch_size * [prompt]]\n",
        "    init_latent = None\n",
        "\n",
        "    if opt.init_img != None and opt.init_img != '':\n",
        "        init_image = load_img(opt.init_img, shape=(opt.W, opt.H)).to(device)\n",
        "        init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
        "        init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
        "\n",
        "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
        "\n",
        "    t_enc = int(opt.strength * opt.ddim_steps)\n",
        "\n",
        "    start_code = None\n",
        "    if opt.fixed_code and init_latent == None:\n",
        "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
        "\n",
        "    images = []\n",
        "    precision_scope = autocast if opt.precision == \"autocast\" else nullcontext\n",
        "    with torch.no_grad():\n",
        "        with precision_scope(\"cuda\"):\n",
        "            with model.ema_scope():\n",
        "                for prompts in data:\n",
        "                    uc = None\n",
        "                    if opt.scale != 1.0:\n",
        "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
        "                    if isinstance(prompts, tuple):\n",
        "                        prompts = list(prompts)\n",
        "                    c = model.get_learned_conditioning(prompts)\n",
        "\n",
        "                    if init_latent != None:\n",
        "                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device))\n",
        "                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
        "                                                unconditional_conditioning=uc,)\n",
        "                    else:\n",
        "\n",
        "                        if opt.sampler == 'klms':\n",
        "                            print(\"Using KLMS sampling\")\n",
        "                            shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                            sigmas = model_wrap.get_sigmas(opt.ddim_steps)\n",
        "                            model_wrap_cfg = CFGDenoiser(model_wrap)\n",
        "                            x = torch.randn([opt.n_samples, *shape], device=device) * sigmas[0]\n",
        "                            extra_args = {'cond': c, 'uncond': uc, 'cond_scale': opt.scale}\n",
        "                            samples = sample_lms(model_wrap_cfg, x, sigmas, extra_args=extra_args, disable=False)\n",
        "                        else:\n",
        "                            shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
        "                            samples, _ = sampler.sample(S=opt.ddim_steps,\n",
        "                                                            conditioning=c,\n",
        "                                                            batch_size=opt.n_samples,\n",
        "                                                            shape=shape,\n",
        "                                                            verbose=False,\n",
        "                                                            unconditional_guidance_scale=opt.scale,\n",
        "                                                            unconditional_conditioning=uc,\n",
        "                                                            eta=opt.ddim_eta,\n",
        "                                                            x_T=start_code)\n",
        "\n",
        "                    x_samples = model.decode_first_stage(samples)\n",
        "                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "                    for x_sample in x_samples:\n",
        "                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
        "                        images.append(Image.fromarray(x_sample.astype(np.uint8)))\n",
        "                        #filepath = os.path.join(opt.outdir, f\"{batch_name}({batch_idx})_{sample_idx:04}.png\")\n",
        "                        #print(f\"Saving to {filepath}\")\n",
        "                        #Image.fromarray(x_sample.astype(np.uint8)).save(filepath)\n",
        "                        sample_idx += 1\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzmVAdZ1-5tE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Waiting for GIMP requests\n",
        "\n",
        "from io import BytesIO\n",
        "import json\n",
        "import base64\n",
        "from flask import Flask, Response, request, abort, make_response\n",
        "#from flask_ngrok import run_with_ngrok\n",
        "from flask_cloudflared import run_with_cloudflared\n",
        "\n",
        "opt.sampler = \"ddim\"\n",
        "opt.ddim_eta = 0.75\n",
        "opt.n_samples = 1\n",
        "opt.outdir = outputs_path\n",
        "\n",
        "init_img = os.path.join(outputs_path, \"init.png\")\n",
        "opt.init_img = init_img\n",
        "\n",
        "os.makedirs(opt.outdir, exist_ok=True)\n",
        "API_VERSION = 3\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/api/img2img\", methods=[\"POST\"])\n",
        "def img2img():\n",
        "    r = request\n",
        "    data = r.data.decode(\"utf-8\")\n",
        "    data = json.loads(data)\n",
        "\n",
        "    api_version = 0\n",
        "\n",
        "    if \"api_version\" in data:\n",
        "       api_version = int(data[\"api_version\"])\n",
        "\n",
        "    if api_version != API_VERSION:\n",
        "       abort(405)\n",
        "\n",
        "    print(\"\\n\")\n",
        "    print(\"Data sent from Gimp\")\n",
        "    print(\"init_strength: \" + str(data[\"init_strength\"]) + \", prompt_strength: \" + str(data[\"prompt_strength\"]) + \", steps: \" + str(data[\"steps\"]) + \", width: \" + str(data[\"width\"]) + \", height: \" + str(data[\"height\"]) + \", prompt: \" + data[\"prompt\"] + \", seed: \" + str(data[\"seed\"]) + \", api_version: \" + str(data[\"api_version\"]))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    img_data = base64.b64decode(data[\"init_img\"])\n",
        "    img_file = open(init_img, \"wb+\")\n",
        "    img_file.write(img_data)\n",
        "    img_file.close()\n",
        "    \n",
        "    opt.W, opt.H = map(lambda x: x - x % 64, (int(data[\"width\"]), int(data[\"height\"])))\n",
        "    opt.strength = max(0.0, min(1.0, 1.0 - float(data[\"init_strength\"])))\n",
        "    opt.guidance_scale = float(data[\"prompt_strength\"])\n",
        "    opt.ddim_steps = int(data[\"steps\"])\n",
        "    opt.prompt = data[\"prompt\"]\n",
        "\n",
        "    print(\"Parameters used for generating\")\n",
        "    print(opt.__dict__)\n",
        "    print(\"\\n\")\n",
        "\n",
        "    imgs_return = []\n",
        "\n",
        "    for counter in range(data[\"image_count\"]):\n",
        "       # initialize\n",
        "       gc.collect()\n",
        "       torch.cuda.empty_cache()\n",
        "       sample_idx = 0\n",
        "    \n",
        "       opt.seed = int(data[\"seed\"]) if int(data[\"seed\"]) != -1 else random.randint(0, 2**32)\n",
        "\n",
        "       img = generate(opt)[0]\n",
        "\n",
        "       img_data = BytesIO()\n",
        "       img.save(img_data, format=\"PNG\")\n",
        "       img_data.seek(0)\n",
        "       img_encoded = base64.b64encode(img_data.read())\n",
        "       img_encoded = img_encoded.decode(\"utf-8\")\n",
        "\n",
        "       img_return = {\"seed\": opt.seed, \"image\": img_encoded}\n",
        "       imgs_return.append(img_return)\n",
        "\n",
        "       print(f\"Used seed: {opt.seed}\")\n",
        "\n",
        "    data_return = {\"images\": imgs_return}\n",
        "    data_return = json.dumps(data_return)\n",
        "\n",
        "    if os.path.exists(init_img):\n",
        "       os.remove(init_img)\n",
        "\n",
        "    response = make_response()\n",
        "    response.headers[\"mimetype\"] = \"application/json\"\n",
        "    response.data = data_return\n",
        "    return response\n",
        "\n",
        "run_with_cloudflared(app)\n",
        "app.run()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "2e35b1f3b2666f0e402b0693dd7493a583002c98361385482aa9f27d8f0f5c89"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
